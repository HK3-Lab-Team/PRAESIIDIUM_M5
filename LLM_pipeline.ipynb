{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    This notebook contains the code to perform LLM-driven categorization of food entries on the Shangai T2DM data.\n",
    "\n",
    "    Running the notebook assumes you have already \n",
    "    - installed all requiredments in `./requirements.txt` \n",
    "    - downloaded the data in the `./data` folder \n",
    "    - set up your LLM API key in `.env` file.\n",
    "    \n",
    "    See README.md for instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_base_path = \"./data\"\n",
    "output_file_name = \"processed_food_diary_entries.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use LLMs, we first need to check the keys are properly set in the .env file. For this analysis, we use an OpenAI LLM (gpt-4o-mini)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Check if the .env file is present. If not, please create the file.\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found\") # Note: no errors must be raised!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need a cache folder to store the output files of the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_folder = \"./LLM_outputs/\"\n",
    "cache_folder = f\"{out_folder}cache_folder/\"\n",
    "os.makedirs(cache_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define structured Pydantic models to represent, validate, and analyze raw food diary entries, including meal classification, portion assessment, and data quality flags. These classes support both LLM input formatting and downstream consistency checks for nutrition-related data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, ValidationInfo, field_validator, model_validator\n",
    "from typing import List, Dict, Optional\n",
    "from enum import Enum\n",
    "\n",
    "class MealType(str, Enum):\n",
    "    \"\"\"\n",
    "    Classification of the meal timing during the day.\n",
    "    \"\"\"\n",
    "    BREAKFAST = \"Breakfast\"\n",
    "    LUNCH = \"Lunch\"\n",
    "    DINNER = \"Dinner\"\n",
    "\n",
    "class PortionSize(str, Enum):\n",
    "    \"\"\"\n",
    "    Classification of portion sizes relative to standard recommendations.\n",
    "    \"\"\"\n",
    "    LOW = \"low\"  # Below recommended range\n",
    "    AVERAGE = \"average\"  # Within recommended range\n",
    "    HIGH = \"high\"  # Above recommended range but plausible\n",
    "    TYPING_ERROR = \"typing_error\"  # Likely data entry error\n",
    "\n",
    "class CaloricDensity(str, Enum):\n",
    "    \"\"\"\n",
    "    Overall caloric assessment of the meal.\n",
    "    \"\"\"\n",
    "    LOW = \"low\"  # Insufficient calories for meal type\n",
    "    AVERAGE = \"average\"  # Appropriate caloric content\n",
    "    HIGH = \"high\"  # Higher than recommended but plausible\n",
    "    UNREALISTIC = \"unrealistic\"  # Implausible caloric content\n",
    "\n",
    "class DataQualityIssue(str, Enum):\n",
    "    \"\"\"\n",
    "    Common data quality issues identified in the entries.\n",
    "    \"\"\"\n",
    "    MISSING_QUANTITY = \"missing_quantity\"  # Amounts not specified\n",
    "    MISSING_UNIT = \"missing_unit\"  # Units not specified\n",
    "    FORMATTING_ERROR = \"formatting_error\"  # Text formatting issues\n",
    "    AMBIGUOUS_DESCRIPTION = \"ambiguous_description\"  # Unclear descriptions\n",
    "    INCOMPLETE_ENTRY = \"incomplete_entry\"  # Missing major components\n",
    "    UNREALISTIC_VALUE = \"unrealistic_value\"  # Implausible amounts\n",
    "    TYPING_ERROR = \"typing_error\"  # Likely typing/data entry errors\n",
    "    VALID = \"valid\"  # No issues detected\n",
    "\n",
    "class UnprocessedFoodDiaryEntry(BaseModel):\n",
    "    \"\"\"\n",
    "    Raw food diary entry containing the original description and patient context.\n",
    "    This is used as input for LLM processing.\n",
    "    \"\"\"\n",
    "    event_description: str = Field(\n",
    "        ...,\n",
    "        description=\"Raw text description of the meal, typically containing food items and their quantities in various formats\",\n",
    "        examples=[\"Rice 185 g\\nTomato and egg soup 200 g\\nBraised eggplant 50 g\"]\n",
    "    )\n",
    "    event_type: MealType = Field(\n",
    "        ...,\n",
    "        description=\"The type of meal or eating event\"\n",
    "    )\n",
    "    age: int = Field(\n",
    "        ...,\n",
    "        ge=0,\n",
    "        le=120,\n",
    "        description=\"Patient's age in years\"\n",
    "    )\n",
    "    height: float = Field(\n",
    "        ...,\n",
    "        ge=0.5,\n",
    "        le=2.5,\n",
    "        description=\"Patient's height in meters\"\n",
    "    )\n",
    "    weight: float = Field(\n",
    "        ...,\n",
    "        ge=20.0,\n",
    "        le=300.0,\n",
    "        description=\"Patient's weight in kilograms\"\n",
    "    )\n",
    "    bmi: float = Field(\n",
    "        ...,\n",
    "        ge=10.0,\n",
    "        le=100.0,\n",
    "        description=\"Patient's Body Mass Index (weight/height²)\"\n",
    "    )\n",
    "    is_female: bool = Field(\n",
    "        ...,\n",
    "        description=\"Patient's biological sex (True for female, False for male)\"\n",
    "    )\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"\n",
    "        Create a markdown formatted string representation suitable for LLM input.\n",
    "        \"\"\"\n",
    "        lines = [\n",
    "            f\"## Food Diary Entry - {self.event_type}\",\n",
    "            \"\",\n",
    "            \"### Patient Information\",\n",
    "            f\"- Age: {self.age} years\",\n",
    "            f\"- Gender: {'Female' if self.is_female else 'Male'}\",\n",
    "            f\"- BMI: {self.bmi:.1f} kg/m²\",\n",
    "            \"\",\n",
    "            \"### Raw Food Description\",\n",
    "            f\"{self.event_description}\"\n",
    "        ]\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "class FoodItemWithAnalysis(BaseModel):\n",
    "    \"\"\"\n",
    "    Combined food item and portion analysis.\n",
    "    \"\"\"\n",
    "    name: str = Field(\n",
    "        ...,\n",
    "        description=\"The name of the food item, including cooking method when known (e.g., 'Steamed rice', 'Fried chicken')\",\n",
    "        min_length=1\n",
    "    )\n",
    "    amount: float = Field(\n",
    "        ...,\n",
    "        description=\"The numerical quantity of the food item\",\n",
    "        gt=0\n",
    "    )\n",
    "    unit: str = Field(\n",
    "        ...,\n",
    "        description=\"The unit of measurement (e.g., 'g' for grams, 'ml' for milliliters, 'piece')\",\n",
    "        min_length=1\n",
    "    )\n",
    "    portion_size: PortionSize = Field(\n",
    "        ...,\n",
    "        description=\"Assessment of this item's portion size compared to standard recommendations\"\n",
    "    )\n",
    "    standard_range: str = Field(\n",
    "        ...,\n",
    "        description=\"Reference range for this food type (e.g., '150-300g for rice')\"\n",
    "    )\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"String representation with portion analysis\"\"\"\n",
    "        return f\"{self.name} ({self.amount} {self.unit}, {self.portion_size})\"\n",
    "\n",
    "class ProcessedFoodDiaryEntry(UnprocessedFoodDiaryEntry):\n",
    "    \"\"\"\n",
    "    Processed food diary entry with detailed analysis fields.\n",
    "    \"\"\"\n",
    "    items: List[FoodItemWithAnalysis] = Field(\n",
    "        ...,\n",
    "        description=\"List of food items with portion analysis\",\n",
    "        min_items=1\n",
    "    )\n",
    "    \n",
    "    meal_caloric_density: CaloricDensity = Field(\n",
    "        ...,\n",
    "        description=\"Overall assessment of the meal's caloric content\"\n",
    "    )\n",
    "    \n",
    "    data_quality_issues: List[DataQualityIssue] = Field(\n",
    "        ...,\n",
    "        description=\"List of identified data quality issues in the entry\",\n",
    "        min_items=1\n",
    "    )\n",
    "    \n",
    "    is_meal_complete: bool = Field(\n",
    "        ...,\n",
    "        description=\"Whether the meal appears to be a complete record (vs. partial logging)\"\n",
    "    )\n",
    "    \n",
    "    has_realistic_portions: bool = Field(\n",
    "        ...,\n",
    "        description=\"Whether all portions are within normal ranges\"\n",
    "    )\n",
    "    \n",
    "    needs_nutrition_review: bool = Field(\n",
    "        ...,\n",
    "        description=\"Flag for entries that might need dietary intervention\"\n",
    "    )\n",
    "\n",
    "    should_reject_sample: bool = Field(\n",
    "        ...,\n",
    "        description=\"\"\"Final decision on sample rejection. Only reject if:\n",
    "        - Clearly impossible amounts (e.g., 1500g rice)\n",
    "        - Missing critical information (quantities/units)\n",
    "        - Completely ambiguous descriptions and incomplete\n",
    "        - Formatting errors making interpretation impossible and incomplete\n",
    "        Do NOT reject for:\n",
    "        - Small but plausible portions\n",
    "        - Single item snacks\n",
    "        - Diet-appropriate portions\n",
    "        - Cultural portion variations\"\"\"\n",
    "    )\n",
    "    \n",
    "    extra_info: str = Field(\n",
    "        ...,\n",
    "        description=\"Additional context, explanations of issues, or analyst notes\"\n",
    "    )\n",
    "\n",
    "    @field_validator(\"age\", \"height\", \"weight\", \"bmi\", \"is_female\", \"event_type\", mode='after')\n",
    "    @classmethod\n",
    "    def validate_unchanged_fields(cls, value: any, info: ValidationInfo) -> any:\n",
    "        \"\"\"Validates that fields from the original entry remain unchanged.\"\"\"\n",
    "        if info.context is None:\n",
    "            return value\n",
    "            \n",
    "        original_entry: Optional[Dict] = info.context.get('original_entry')\n",
    "        if original_entry is None:\n",
    "            return value\n",
    "            \n",
    "        field_name = info.field_name\n",
    "        original_value = original_entry.get(field_name)\n",
    "        \n",
    "        if original_value is not None and value != original_value:\n",
    "            raise ValueError(\n",
    "                f\"Field '{field_name}' has been modified. \"\n",
    "                f\"Original value: {original_value}, \"\n",
    "                f\"New value: {value}. \"\n",
    "                \"Patient information fields must remain unchanged during processing.\"\n",
    "            )\n",
    "        \n",
    "        return value\n",
    "\n",
    "    @model_validator(mode='after')\n",
    "    def validate_meal(self) -> 'ProcessedFoodDiaryEntry':\n",
    "        \"\"\"Validate overall meal consistency\"\"\"\n",
    "        # Set has_realistic_portions based on portion sizes\n",
    "        unrealistic_portions = any(\n",
    "            item.portion_size == PortionSize.TYPING_ERROR \n",
    "            for item in self.items\n",
    "        )\n",
    "        if unrealistic_portions != (not self.has_realistic_portions):\n",
    "            self.has_realistic_portions = not unrealistic_portions\n",
    "\n",
    "        # Determine if sample should be rejected\n",
    "        should_reject = any([\n",
    "            # Data entry errors\n",
    "            unrealistic_portions,\n",
    "            # Missing critical information\n",
    "            DataQualityIssue.MISSING_QUANTITY in self.data_quality_issues,\n",
    "            DataQualityIssue.MISSING_UNIT in self.data_quality_issues,\n",
    "            # Formatting issues\n",
    "            DataQualityIssue.FORMATTING_ERROR in self.data_quality_issues \n",
    "            and not self.is_meal_complete,  # Only reject if incomplete\n",
    "            # Ambiguous entries\n",
    "            DataQualityIssue.AMBIGUOUS_DESCRIPTION in self.data_quality_issues \n",
    "            and not self.is_meal_complete  # Only reject if incomplete\n",
    "        ])\n",
    "\n",
    "        if should_reject != self.should_reject_sample:\n",
    "            self.should_reject_sample = should_reject\n",
    "\n",
    "        # Ensure UNREALISTIC_VALUE in issues if unrealistic portions\n",
    "        if unrealistic_portions and DataQualityIssue.UNREALISTIC_VALUE not in self.data_quality_issues:\n",
    "            self.data_quality_issues.append(DataQualityIssue.UNREALISTIC_VALUE)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"\n",
    "        Create a markdown formatted string representation suitable for review.\n",
    "        \"\"\"\n",
    "        lines = [\n",
    "            f\"## Food Diary Entry - {self.event_type}\",\n",
    "            \"\",\n",
    "            \"### Patient Information\",\n",
    "            f\"- Age: {self.age} years\",\n",
    "            f\"- Gender: {'Female' if self.is_female else 'Male'}\",\n",
    "            f\"- BMI: {self.bmi:.1f} kg/m²\",\n",
    "            \"\",\n",
    "            \"### Original Description\",\n",
    "            self.event_description,\n",
    "            \"\",\n",
    "            \"### Structured Food Items\"\n",
    "        ]\n",
    "        \n",
    "        # Add each food item with its analysis\n",
    "        for item in self.items:\n",
    "            lines.append(f\"- {str(item)}\")\n",
    "            lines.append(f\"  * Standard Range: {item.standard_range}\")\n",
    "            \n",
    "        # Add assessment section\n",
    "        lines.extend([\n",
    "            \"\",\n",
    "            \"### Meal Assessment\",\n",
    "            f\"- Caloric Density: {self.meal_caloric_density}\",\n",
    "            f\"- Data Quality Issues: {', '.join(str(issue) for issue in self.data_quality_issues)}\",\n",
    "            f\"- Complete Meal: {'Yes' if self.is_meal_complete else 'No'}\",\n",
    "            f\"- Realistic Portions: {'Yes' if self.has_realistic_portions else 'No'}\",\n",
    "            f\"- Needs Nutrition Review: {'Yes' if self.needs_nutrition_review else 'No'}\",\n",
    "            f\"- Should Reject Sample: {'Yes' if self.should_reject_sample else 'No'}\",\n",
    "            \"\",\n",
    "            \"### Additional Information\",\n",
    "            self.extra_info\n",
    "        ])\n",
    "        \n",
    "        return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the system and user prompts for the LLM to process the food diary entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_system_prompt(unprocessed_schema: dict, processed_schema: dict) -> str:\n",
    "    \"\"\"\n",
    "    Creates the system prompt with proper schema references and mixed items handling.\n",
    "    \"\"\"\n",
    "    return f'''You are a specialized AI trained to process food diary entries by extracting structured food items from text descriptions and performing detailed nutritional analysis. Your task is to transform unprocessed food diary entries into processed entries with validated food items.\n",
    "\n",
    "INPUT SCHEMA:\n",
    "{str(unprocessed_schema)}\n",
    "\n",
    "OUTPUT SCHEMA:\n",
    "{str(processed_schema)}\n",
    "\n",
    "Your primary responsibilities are:\n",
    "\n",
    "1. EXTRACTION AND STRUCTURING:\n",
    "   - Extract food items with their quantities\n",
    "   - Keep mixed items together (do not split amounts)\n",
    "   - Include cooking methods when known\n",
    "   - Maintain complete item descriptions\n",
    "\n",
    "   Mixed Items Handling:\n",
    "   - Keep mixed ingredients as single items with their total weight\n",
    "   - Use complete, descriptive names\n",
    "   Example: \"vegetables and meat stir-fry 150g\" becomes:\n",
    "   {{\n",
    "     \"name\": \"Stir-fried meat with vegetables\",\n",
    "     \"amount\": 150,\n",
    "     \"unit\": \"g\"\n",
    "   }}\n",
    "   NOT:\n",
    "   [\n",
    "     {{\"name\": \"Meat\", \"amount\": 100, \"unit\": \"g\"}},\n",
    "     {{\"name\": \"Vegetables\", \"amount\": 50, \"unit\": \"g\"}}  // Don't split like this\n",
    "   ]\n",
    "\n",
    "2. PORTION SIZE ANALYSIS:\n",
    "   Each food item must be classified as:\n",
    "   - \"low\": Below standard range but plausible (e.g., elderly, diet control)\n",
    "   - \"average\": Within standard range\n",
    "   - \"high\": Above standard range but plausible\n",
    "   - \"typing_error\": Clearly impossible amounts\n",
    "\n",
    "   Standard Ranges (for complete dishes):\n",
    "   * Mixed Rice/Noodle Dishes: 200-400g\n",
    "   * Mixed Meat/Vegetable Dishes: 150-300g\n",
    "   * Soups/Porridges: 200-400ml\n",
    "   * Single Component Items:\n",
    "     - Plain Rice/Pasta/Noodles: 150-300g\n",
    "     - Plain Meat/Fish: 100-200g\n",
    "     - Plain Vegetables: 100-300g\n",
    "     - Fruits: 100-200g\n",
    "   \n",
    "   Adjust ranges for:\n",
    "   - Elderly patients: 50-70% of standard range\n",
    "   - Diet control: 50-70% of standard range\n",
    "   - High BMI patients: lower end of ranges\n",
    "   - Low BMI patients: higher end of ranges\n",
    "\n",
    "3. DATA QUALITY ASSESSMENT:\n",
    "   Use appropriate data_quality_issues:\n",
    "   - MISSING_QUANTITY: No amount specified\n",
    "   - MISSING_UNIT: No unit specified\n",
    "   - FORMATTING_ERROR: Text formatting issues (e.g., missing spaces)\n",
    "   - AMBIGUOUS_DESCRIPTION: Unclear descriptions\n",
    "   - INCOMPLETE_ENTRY: Missing major components\n",
    "   - UNREALISTIC_VALUE: Clearly impossible amounts\n",
    "   - TYPING_ERROR: Data entry errors (e.g., missing decimals)\n",
    "   - VALID: No issues detected\n",
    "\n",
    "4. REJECTION CRITERIA:\n",
    "   Set should_reject_sample = true ONLY for:\n",
    "   - Clearly impossible amounts (e.g., 1500g rice)\n",
    "   - Missing critical information AND incomplete meal\n",
    "   - Completely ambiguous descriptions AND incomplete meal\n",
    "   - Formatting errors making interpretation impossible\n",
    "\n",
    "   Do NOT reject for:\n",
    "   - Small portions (especially for elderly)\n",
    "   - Single item entries that are complete\n",
    "   - High but plausible portions\n",
    "   - Mixed items with shared weights\n",
    "\n",
    "Here are examples showing proper analysis:\n",
    "\n",
    "EXAMPLE 1 - Mixed Items (Valid Entry):\n",
    "Input:\n",
    "{{\n",
    "    \"event_description\": \"Pork and vegetable stir-fry 250g, Rice 150g\",\n",
    "    \"event_type\": \"Lunch\",\n",
    "    \"age\": 45,\n",
    "    \"height\": 1.75,\n",
    "    \"weight\": 70.0,\n",
    "    \"bmi\": 22.9,\n",
    "    \"is_female\": false\n",
    "}}\n",
    "\n",
    "Output:\n",
    "{{\n",
    "    \"event_description\": \"Pork and vegetable stir-fry 250g, Rice 150g\",\n",
    "    \"event_type\": \"Lunch\",\n",
    "    \"age\": 45,\n",
    "    \"height\": 1.75,\n",
    "    \"weight\": 70.0,\n",
    "    \"bmi\": 22.9,\n",
    "    \"is_female\": false,\n",
    "    \"items\": [\n",
    "        {{\n",
    "            \"name\": \"Stir-fried pork with vegetables\",\n",
    "            \"amount\": 250,\n",
    "            \"unit\": \"g\",\n",
    "            \"portion_size\": \"average\",\n",
    "            \"standard_range\": \"150-300g for mixed meat and vegetable dishes\"\n",
    "        }},\n",
    "        {{\n",
    "            \"name\": \"Steamed white rice\",\n",
    "            \"amount\": 150,\n",
    "            \"unit\": \"g\",\n",
    "            \"portion_size\": \"average\",\n",
    "            \"standard_range\": \"150-300g for rice\"\n",
    "        }}\n",
    "    ],\n",
    "    \"meal_caloric_density\": \"average\",\n",
    "    \"data_quality_issues\": [\"valid\"],\n",
    "    \"is_meal_complete\": true,\n",
    "    \"has_realistic_portions\": true,\n",
    "    \"needs_nutrition_review\": false,\n",
    "    \"should_reject_sample\": false,\n",
    "    \"extra_info\": \"Well-balanced meal with appropriate portions. Mixed dish kept as single item with total weight.\"\n",
    "}}\n",
    "\n",
    "EXAMPLE 2 - Elderly Patient (Valid with Low Portions):\n",
    "Input:\n",
    "{{\n",
    "    \"event_description\": \"Mixed vegetable soup with meat 150ml, Small rice 75g\",\n",
    "    \"event_type\": \"Dinner\",\n",
    "    \"age\": 82,\n",
    "    \"height\": 1.55,\n",
    "    \"weight\": 52.0,\n",
    "    \"bmi\": 21.6,\n",
    "    \"is_female\": true\n",
    "}}\n",
    "\n",
    "Output:\n",
    "{{\n",
    "    \"event_description\": \"Mixed vegetable soup with meat 150ml, Small rice 75g\",\n",
    "    \"event_type\": \"Dinner\",\n",
    "    \"age\": 82,\n",
    "    \"height\": 1.55,\n",
    "    \"weight\": 52.0,\n",
    "    \"bmi\": 21.6,\n",
    "    \"is_female\": true,\n",
    "    \"items\": [\n",
    "        {{\n",
    "            \"name\": \"Mixed vegetable and meat soup\",\n",
    "            \"amount\": 150,\n",
    "            \"unit\": \"ml\",\n",
    "            \"portion_size\": \"low\",\n",
    "            \"standard_range\": \"200-400ml for soups, 100-200ml appropriate for elderly\"\n",
    "        }},\n",
    "        {{\n",
    "            \"name\": \"Steamed white rice\",\n",
    "            \"amount\": 75,\n",
    "            \"unit\": \"g\",\n",
    "            \"portion_size\": \"low\",\n",
    "            \"standard_range\": \"150-300g for rice, 50-150g appropriate for elderly\"\n",
    "        }}\n",
    "    ],\n",
    "    \"meal_caloric_density\": \"low\",\n",
    "    \"data_quality_issues\": [\"valid\"],\n",
    "    \"is_meal_complete\": true,\n",
    "    \"has_realistic_portions\": true,\n",
    "    \"needs_nutrition_review\": false,\n",
    "    \"should_reject_sample\": false,\n",
    "    \"extra_info\": \"Portions appropriately sized for elderly patient. Mixed soup kept as single item. While portions are below standard ranges, they are suitable for demographics.\"\n",
    "}}\n",
    "\n",
    "EXAMPLE 3 - Data Entry Error:\n",
    "Input:\n",
    "{{\n",
    "    \"event_description\": \"Rice1500g, Mixed vegetables and pork 50g\",\n",
    "    \"event_type\": \"Breakfast\",\n",
    "    \"age\": 35,\n",
    "    \"height\": 1.65,\n",
    "    \"weight\": 65.0,\n",
    "    \"bmi\": 23.9,\n",
    "    \"is_female\": true\n",
    "}}\n",
    "\n",
    "Output:\n",
    "{{\n",
    "    \"event_description\": \"Rice1500g, Mixed vegetables and pork 50g\",\n",
    "    \"event_type\": \"Breakfast\",\n",
    "    \"age\": 35,\n",
    "    \"height\": 1.65,\n",
    "    \"weight\": 65.0,\n",
    "    \"bmi\": 23.9,\n",
    "    \"is_female\": true,\n",
    "    \"items\": [\n",
    "        {{\n",
    "            \"name\": \"Rice\",\n",
    "            \"amount\": 1500,\n",
    "            \"unit\": \"g\",\n",
    "            \"portion_size\": \"typing_error\",\n",
    "            \"standard_range\": \"150-300g for rice\"\n",
    "        }},\n",
    "        {{\n",
    "            \"name\": \"Mixed vegetables and pork stir-fry\",\n",
    "            \"amount\": 50,\n",
    "            \"unit\": \"g\",\n",
    "            \"portion_size\": \"low\",\n",
    "            \"standard_range\": \"150-300g for mixed dishes\"\n",
    "        }}\n",
    "    ],\n",
    "    \"meal_caloric_density\": \"unrealistic\",\n",
    "    \"data_quality_issues\": [\"formatting_error\", \"typing_error\", \"unrealistic_value\"],\n",
    "    \"is_meal_complete\": false,\n",
    "    \"has_realistic_portions\": false,\n",
    "    \"needs_nutrition_review\": true,\n",
    "    \"should_reject_sample\": true,\n",
    "    \"extra_info\": \"Clear data entry error in rice portion (1500g, likely missing decimal). Mixed dish kept as single item but portion is low. Formatting error in rice entry (no space).\"\n",
    "}}\n",
    "\n",
    "EXAMPLE 4 - Complex Mixed Items:\n",
    "Input:\n",
    "{{\n",
    "    \"event_description\": \"Stir-fried rice with shrimp, pork and vegetables 300g\\\\nCabbage and mushroom soup 200ml\",\n",
    "    \"event_type\": \"Lunch\",\n",
    "    \"age\": 55,\n",
    "    \"height\": 1.70,\n",
    "    \"weight\": 68.0,\n",
    "    \"bmi\": 23.5,\n",
    "    \"is_female\": false\n",
    "}}\n",
    "\n",
    "Output:\n",
    "{{\n",
    "    \"event_description\": \"Stir-fried rice with shrimp, pork and vegetables 300g\\\\nCabbage and mushroom soup 200ml\",\n",
    "    \"event_type\": \"Lunch\",\n",
    "    \"age\": 55,\n",
    "    \"height\": 1.70,\n",
    "    \"weight\": 68.0,\n",
    "    \"bmi\": 23.5,\n",
    "    \"is_female\": false,\n",
    "    \"items\": [\n",
    "        {{\n",
    "            \"name\": \"Mixed stir-fried rice with shrimp, pork and vegetables\",\n",
    "            \"amount\": 300,\n",
    "            \"unit\": \"g\",\n",
    "            \"portion_size\": \"average\",\n",
    "            \"standard_range\": \"200-400g for mixed rice dishes\"\n",
    "        }},\n",
    "        {{\n",
    "            \"name\": \"Cabbage and mushroom soup\",\n",
    "            \"amount\": 200,\n",
    "            \"unit\": \"ml\",\n",
    "            \"portion_size\": \"average\",\n",
    "            \"standard_range\": \"200-400ml for soups\"\n",
    "        }}\n",
    "    ],\n",
    "    \"meal_caloric_density\": \"average\",\n",
    "    \"data_quality_issues\": [\"valid\"],\n",
    "    \"is_meal_complete\": true,\n",
    "    \"has_realistic_portions\": true,\n",
    "    \"needs_nutrition_review\": false,\n",
    "    \"should_reject_sample\": false,\n",
    "    \"extra_info\": \"Well-balanced meal with appropriate portions. Complex mixed dishes kept as single items with clear descriptions. Portions suitable for patient demographics.\"\n",
    "}}\n",
    "\n",
    "Remember:\n",
    "1. Always keep mixed items together with their total weight\n",
    "2. Consider patient demographics for portion assessment\n",
    "3. Use descriptive names that include all components\n",
    "4. Document any assumptions in extra_info\n",
    "5. Only reject samples that are truly unusable\n",
    "\n",
    "Your goal is to create accurate, structured data while properly handling mixed items and maintaining consistency in portion analysis.\n",
    "'''\n",
    "\n",
    "def create_user_prompt(unprocessed_entry: UnprocessedFoodDiaryEntry) -> str:\n",
    "    \"\"\"\n",
    "    Creates a formatted user prompt from an unprocessed food diary entry.\n",
    "    \"\"\"\n",
    "    return f'''Please analyze this food diary entry and convert it to a structured format with detailed nutritional analysis. Consider the patient's context:\n",
    "- Age: {unprocessed_entry.age} years\n",
    "- BMI: {unprocessed_entry.bmi:.1f} kg/m²\n",
    "- Gender: {'Female' if unprocessed_entry.is_female else 'Male'}\n",
    "\n",
    "Here is the entry in both human-readable and JSON format:\n",
    "\n",
    "HUMAN READABLE FORMAT:\n",
    "{str(unprocessed_entry)}\n",
    "\n",
    "JSON FORMAT:\n",
    "{unprocessed_entry.model_dump_json(indent=2)}\n",
    "\n",
    "For each food item provide:\n",
    "1. Standardized name (including cooking method when known)\n",
    "2. Amount and unit\n",
    "3. Portion size assessment (\"low\", \"average\", \"high\", or \"typing_error\")\n",
    "4. Standard portion range reference\n",
    "\n",
    "Analyze the overall meal for:\n",
    "1. Completeness and balance\n",
    "2. Caloric density\n",
    "3. Data quality issues\n",
    "4. Need for nutritional review\n",
    "5. Potential rejection criteria\n",
    "\n",
    "Consider the patient's age and BMI when assessing portions. Format your response as a valid JSON object matching the output schema provided. Include only the JSON object, no explanatory text.'''\n",
    "\n",
    "system_prompt = create_system_prompt(unprocessed_schema=UnprocessedFoodDiaryEntry.model_json_schema(),processed_schema=ProcessedFoodDiaryEntry.model_json_schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example usage showing both prompt creation and entry processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample test cases\n",
    "test_cases = [\n",
    "    # Case 1: Elderly patient with small portions\n",
    "    {\n",
    "        'event_description': 'Rice 185 g\\nTomato and egg soup 200 g\\nBraised eggplant 50 g\\nBraised meat 25 g\\nVegetable salad 90 g\\nHairtail 30 g',\n",
    "        'event_type': 'Lunch',\n",
    "        'age': 77,\n",
    "        'height': 1.5,\n",
    "        'weight': 56.0,\n",
    "        'bmi': 24.89,\n",
    "        'is_female': True\n",
    "    },\n",
    "    # Case 2: Data entry error example\n",
    "    {\n",
    "        'event_description': 'Rice1500g\\nVegetables 50g',\n",
    "        'event_type': 'Breakfast',\n",
    "        'age': 35,\n",
    "        'height': 1.65,\n",
    "        'weight': 65.0,\n",
    "        'bmi': 23.9,\n",
    "        'is_female': True\n",
    "    },\n",
    "    # Case 3: Mixed item example\n",
    "    {\n",
    "        'event_description': 'Vegetable and pork stir-fry 150g',\n",
    "        'event_type': 'Dinner',\n",
    "        'age': 45,\n",
    "        'height': 1.70,\n",
    "        'weight': 68.0,\n",
    "        'bmi': 23.5,\n",
    "        'is_female': False\n",
    "    }\n",
    "]\n",
    "\n",
    "# Process each test case\n",
    "for i, sample_data in enumerate(test_cases, 1):\n",
    "    print(f\"\\n{'='*80}\\nTEST CASE {i}\\n{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Create unprocessed entry\n",
    "        unprocessed_entry = UnprocessedFoodDiaryEntry(**sample_data)\n",
    "        print(\"\\nUNPROCESSED ENTRY:\")\n",
    "        print(unprocessed_entry)\n",
    "        \n",
    "        # 2. Create prompts (what you'd send to LLM)\n",
    "        unprocessed_schema = UnprocessedFoodDiaryEntry.model_json_schema()\n",
    "        processed_schema = ProcessedFoodDiaryEntry.model_json_schema()\n",
    "        \n",
    "        system_prompt = create_system_prompt(unprocessed_schema, processed_schema)\n",
    "        user_prompt = create_user_prompt(unprocessed_entry)\n",
    "        \n",
    "        print(\"\\nPROMPTS CREATED SUCCESSFULLY\")\n",
    "        \n",
    "        # 3. Example processed data (simulating LLM output)\n",
    "        if i == 1:  # Elderly patient case\n",
    "            processed_data = {\n",
    "                **sample_data,\n",
    "                'items': [\n",
    "                    {\n",
    "                        \"name\": \"Steamed white rice\",\n",
    "                        \"amount\": 185,\n",
    "                        \"unit\": \"g\",\n",
    "                        \"portion_size\": \"average\",\n",
    "                        \"standard_range\": \"150-300g for rice, reduced range appropriate for elderly\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"Tomato and egg soup\",\n",
    "                        \"amount\": 200,\n",
    "                        \"unit\": \"g\",\n",
    "                        \"portion_size\": \"average\",\n",
    "                        \"standard_range\": \"200-400ml for soup, 100-200ml appropriate for elderly\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"Braised eggplant\",\n",
    "                        \"amount\": 50,\n",
    "                        \"unit\": \"g\",\n",
    "                        \"portion_size\": \"low\",\n",
    "                        \"standard_range\": \"100-300g for vegetables, 50-100g appropriate for elderly\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"Braised meat\",\n",
    "                        \"amount\": 25,\n",
    "                        \"unit\": \"g\",\n",
    "                        \"portion_size\": \"low\",\n",
    "                        \"standard_range\": \"100-200g for meat, 25-100g appropriate for elderly\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"Fresh vegetable salad\",\n",
    "                        \"amount\": 90,\n",
    "                        \"unit\": \"g\",\n",
    "                        \"portion_size\": \"low\",\n",
    "                        \"standard_range\": \"100-300g for vegetables, 50-100g appropriate for elderly\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"Steamed hairtail fish\",\n",
    "                        \"amount\": 30,\n",
    "                        \"unit\": \"g\",\n",
    "                        \"portion_size\": \"low\",\n",
    "                        \"standard_range\": \"100-200g for fish, 25-100g appropriate for elderly\"\n",
    "                    }\n",
    "                ],\n",
    "                'meal_caloric_density': \"low\",\n",
    "                'data_quality_issues': [\"valid\"],\n",
    "                'is_meal_complete': True,\n",
    "                'has_realistic_portions': True,\n",
    "                'needs_nutrition_review': True,\n",
    "                'should_reject_sample': False,\n",
    "                'extra_info': (\n",
    "                    \"Multiple small portions appropriate for elderly patient (age 77). \"\n",
    "                    \"While portions are below standard ranges, they are within acceptable ranges \"\n",
    "                    \"for elderly individuals with potentially reduced appetite. \"\n",
    "                    \"Meal is complete with good variety (carbs, proteins, vegetables) but \"\n",
    "                    \"nutritional review recommended to ensure caloric needs are being met. \"\n",
    "                    \"Consider patient's BMI (24.89) which is healthy, suggesting these portion \"\n",
    "                    \"sizes may be appropriate for maintaining weight.\"\n",
    "                )\n",
    "            }\n",
    "        elif i == 2:  # Data entry error case\n",
    "            processed_data = {\n",
    "                **sample_data,\n",
    "                'items': [\n",
    "                    {\n",
    "                        \"name\": \"Rice\",\n",
    "                        \"amount\": 1500,\n",
    "                        \"unit\": \"g\",\n",
    "                        \"portion_size\": \"typing_error\",\n",
    "                        \"standard_range\": \"150-300g for rice\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"Mixed vegetables\",\n",
    "                        \"amount\": 50,\n",
    "                        \"unit\": \"g\",\n",
    "                        \"portion_size\": \"low\",\n",
    "                        \"standard_range\": \"100-300g for vegetables\"\n",
    "                    }\n",
    "                ],\n",
    "                'meal_caloric_density': \"unrealistic\",\n",
    "                'data_quality_issues': [\"formatting_error\", \"unrealistic_value\"],\n",
    "                'is_meal_complete': False,\n",
    "                'has_realistic_portions': False,\n",
    "                'needs_nutrition_review': True,\n",
    "                'should_reject_sample': True,\n",
    "                'extra_info': \"Clear data entry error: 1500g of rice is ~7-8 meals worth. Also formatting error (no space between 'Rice' and '1500g').\"\n",
    "            }\n",
    "        else:  # Mixed item case\n",
    "            processed_data = {\n",
    "                **sample_data,\n",
    "                'items': [\n",
    "                    {\n",
    "                        \"name\": \"Stir-fried mixed vegetables\",\n",
    "                        \"amount\": 100,\n",
    "                        \"unit\": \"g\",\n",
    "                        \"portion_size\": \"average\",\n",
    "                        \"standard_range\": \"100-300g for vegetables\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"Stir-fried pork\",\n",
    "                        \"amount\": 50,\n",
    "                        \"unit\": \"g\",\n",
    "                        \"portion_size\": \"low\",\n",
    "                        \"standard_range\": \"100-200g for meat\"\n",
    "                    }\n",
    "                ],\n",
    "                'meal_caloric_density': \"low\",\n",
    "                'data_quality_issues': [\"ambiguous_description\"],\n",
    "                'is_meal_complete': True,\n",
    "                'has_realistic_portions': True,\n",
    "                'needs_nutrition_review': False,\n",
    "                'should_reject_sample': False,\n",
    "                'extra_info': \"Split 150g total between vegetables (100g) and meat (50g) based on typical proportions. While individual portions are low, the combination forms a reasonable meal.\"\n",
    "            }\n",
    "\n",
    "        # 4. Create and validate processed entry\n",
    "        processed_entry = ProcessedFoodDiaryEntry(**processed_data)\n",
    "        print(\"\\nPROCESSED ENTRY:\")\n",
    "        print(processed_entry)\n",
    "        \n",
    "        # 5. Validate with context\n",
    "        processed_with_context = ProcessedFoodDiaryEntry.model_validate(\n",
    "            processed_data,\n",
    "            context=unprocessed_entry.model_dump()\n",
    "        )\n",
    "        \n",
    "        print(\"\\nValidation with context successful!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing test case {i}:\")\n",
    "        print(f\"{'='*40}\")\n",
    "        print(f\"Error type: {type(e).__name__}\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        print(f\"{'='*40}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(system_prompt)\n",
    "print(create_user_prompt(unprocessed_entry))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply this approach to the Shanghai T2DM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.data_preprocess.cgm_data_class import ChineseCGMData\n",
    "import polars as pl\n",
    "import os\n",
    "\n",
    "# Load the data by instantiating the ChineseCGMData class. \n",
    "# When calling this class, we can expect the dtype of some columns to be difficult to infer. Python will fall back to strings.\n",
    "chinese_data = ChineseCGMData(local_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = chinese_data.get_all_events_cgm_data(before_offset=\"-1h\",after_offset=\"3h\")\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df = chinese_data.get_all_events_cgm_data(before_offset=\"-12h\",after_offset=\"12h\")\n",
    "events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_columns = [\"Patient Number\",\"Gender (Female=1, Male=2)\",\"Age (years)\",\"Height (m)\",\"Weight (kg)\",\"BMI (kg/m2)\"]\n",
    "patients_demographics = chinese_data.df_metadata.select(pl.col(demographics_columns)).unique()\n",
    "patients_demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_df = events_df.filter(pl.col(\"event_description\")!=\"data not available\",pl.col(\"event_description\").is_not_null())\n",
    "labelled_df = labelled_df.select(pl.struct([pl.col(\"event_description\"),pl.col(\"Patient Number\"),pl.col(\"event_type\"),pl.col(\"event_id\")]).alias(\"event_struct\")).unique().unnest(\"event_struct\")\n",
    "food_df = labelled_df.join(patients_demographics,on=\"Patient Number\",how=\"left\")\n",
    "simpler_names = {\"Gender (Female=1, Male=2)\":\"gender\",\"Age (years)\":\"age\",\"Height (m)\":\"height\",\"Weight (kg)\":\"weight\",\"BMI (kg/m2)\":\"bmi\"}\n",
    "food_df = food_df.rename(simpler_names)\n",
    "#change gender to bool with 1=female, 0=male and rename column to is_female\n",
    "food_df = food_df.with_columns(pl.when(pl.col(\"gender\")==1).then(True).otherwise(False).alias(\"is_female\"))\n",
    "print(food_df.columns)\n",
    "food_dicts = food_df.select(pl.struct(pl.all().exclude(\"Patient Number\",\"gender\",\"event_id\")).alias(\"event_struct\"))[\"event_struct\"].to_list()\n",
    "events_id = food_df[\"event_id\"].to_list()\n",
    "\n",
    "events_dictionary = dict(zip(events_id,food_dicts))\n",
    "print(events_dictionary)\n",
    "print(len(events_dictionary),len(food_dicts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we reformat the data with the UnprocessedFoodDiaryEntry class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = []\n",
    "errors = []\n",
    "for food_dict in food_dicts:\n",
    "    try:\n",
    "        entry = UnprocessedFoodDiaryEntry(**food_dict)\n",
    "        entries.append(entry)\n",
    "    except ValueError as e:\n",
    "        errors.append(e)\n",
    "        print(f\"Validation error: {e}\")\n",
    "        print(f\"Original event_description: {food_dict['event_description']}\")\n",
    "\n",
    "print(f\"Total number of successful entries: {len(entries)} over {len(food_dicts)} with {len(errors)} errors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validated_events_dictionary: Dict[str,UnprocessedFoodDiaryEntry] = {}\n",
    "for event_id, dict_entry in events_dictionary.items():\n",
    "    validated_events_dictionary[event_id] = UnprocessedFoodDiaryEntry.model_validate(dict_entry)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run the model on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading functions from the LLM inference module\n",
    "from scripts.inference.message_models import LLMPromptContext, LLMConfig, StructuredTool, LLMOutput\n",
    "from scripts.inference.parallel_inference import ParallelAIUtilities, RequestLimits\n",
    "\n",
    "oai_request_limits = RequestLimits(max_requests_per_minute=5000, max_tokens_per_minute=20000000)\n",
    "ai_utils = ParallelAIUtilities(oai_request_limits=oai_request_limits,cache_folder=cache_folder)\n",
    "\n",
    "oai_config = LLMConfig(client=\"openai\",model=\"gpt-4o-mini\",response_format=\"tool\",max_tokens=2000)\n",
    "structured_tool = StructuredTool(json_schema=ProcessedFoodDiaryEntry.model_json_schema(),schema_name=\"process_food_diary_entry\",schema_description=\"Process a food diary entry into a structured format\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating prompts to data for parallel processing\n",
    "prompts = [LLMPromptContext(id=event_id,system_string=system_prompt,new_message=create_user_prompt(entry),llm_config=oai_config) for event_id,entry in validated_events_dictionary.items()]\n",
    "print(\"Here's an example input for the LLM:\")\n",
    "for message in prompts[0].messages:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try with just one prompt\n",
    "single_prompt = prompts[0]\n",
    "result = await ai_utils.run_parallel_ai_completion([single_prompt])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = False # This avoids reprocessing the same events if the notebook is run multiple times\n",
    "\n",
    "\n",
    "if not os.path.exists(f\"{out_folder}{output_file_name}\") and not overwrite:\n",
    "    completion_results = await ai_utils.run_parallel_ai_completion(prompts)\n",
    "else:\n",
    "    completion_results = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "completion_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if completion_results is not None:\n",
    "    processed_dict : Dict[str,ProcessedFoodDiaryEntry] = {}\n",
    "    invalid_food_items  : Dict[str,ProcessedFoodDiaryEntry] = {}\n",
    "    for result in completion_results:\n",
    "        if result.json_object is not None:\n",
    "            try:\n",
    "                unprocessed_input = validated_events_dictionary[result.source_id]\n",
    "                processed_dict[result.source_id] = ProcessedFoodDiaryEntry.model_validate(result.json_object.object,context=unprocessed_input.model_dump())\n",
    "                if  processed_dict[result.source_id].should_reject_sample:\n",
    "                    invalid_food_items[result.source_id] = processed_dict[result.source_id]\n",
    "            except ValueError as e:\n",
    "                errors.append(e)\n",
    "                print(f\"Validation error: {e}\")\n",
    "                print(f\"Original event_description: {validated_events_dictionary[result.source_id].event_description}\")\n",
    "                print(f\"object_pre_validation: {result.json_object.object}\")\n",
    "        else:\n",
    "            errors.append(f\"No JSON object found for event {result.source_id}\")\n",
    "    for id,item in invalid_food_items.items():\n",
    "        print(f\"Event ID: {id}\")\n",
    "        print(f\"Event Description: {item.event_description}\")\n",
    "        print(f\"Extra Info: {item.extra_info}\")\n",
    "        print(\"\\n\")\n",
    "    print(f\"Total number of successful entries: {len(processed_dict)} over {len(validated_events_dictionary)} with {len(errors)} errors and {len(invalid_food_items)} invalid food items\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the results in a parquet file\n",
    "if completion_results is not None:\n",
    "    model_dump_dict : Dict[str,Dict] = {}\n",
    "    for id,item in processed_dict.items():\n",
    "        model_dump_dict[id] = item.model_dump()\n",
    "\n",
    "    input_dict = {\"event_id\":list(model_dump_dict.keys()), \"event_analysis_struct\":list(model_dump_dict.values())}\n",
    "\n",
    "    output_df = pl.DataFrame(input_dict).unnest(\"event_analysis_struct\")\n",
    "    output_df.write_parquet(f\"{out_folder}{output_file_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the LLM output, as it is done in `main_analyses.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved file\n",
    "loaded_df = pl.read_parquet(f\"{out_folder}{output_file_name}\")\n",
    "loaded_df[\"should_reject_sample\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calories_df = loaded_df.filter(pl.col(\"should_reject_sample\") == False).select(pl.col(\"event_id\"),pl.col(\"meal_caloric_density\"))\n",
    "events_with_calories = events_df.join(calories_df,on=\"event_id\",how=\"left\").filter(pl.col(\"meal_caloric_density\").is_in([\"low\",\"average\"]))\n",
    "print(events_with_calories.shape)\n",
    "print(events_with_calories[\"meal_caloric_density\"].value_counts())\n",
    "events_with_calories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import polars as pl\n",
    "from typing import Optional\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "def calculate_hourly_p_values(group1_data, group2_data, standard_times):\n",
    "    \"\"\"\n",
    "    Calculate hourly p-values for two groups of CGM data aligned to event time.\n",
    "    \n",
    "    Args:\n",
    "        group1_data: List of (times, values) tuples for group 1\n",
    "        group2_data: List of (times, values) tuples for group 2\n",
    "        standard_times: Array of standardized time points\n",
    "    \n",
    "    Returns:\n",
    "        Array of p-values for each hour relative to the event\n",
    "    \"\"\"\n",
    "    hours = np.arange(int(standard_times[0] / 60), int(standard_times[-1] / 60) + 1)\n",
    "    p_values = []\n",
    "    \n",
    "    for hour in hours:\n",
    "        start_time = hour * 60\n",
    "        end_time = (hour + 1) * 60\n",
    "        \n",
    "        group1_values = []\n",
    "        group2_values = []\n",
    "        \n",
    "        for times, values in group1_data:\n",
    "            times = np.array(times)\n",
    "            values = np.array(values)\n",
    "            mask = (times >= start_time) & (times < end_time)\n",
    "            group1_values.extend(values[mask])\n",
    "        \n",
    "        for times, values in group2_data:\n",
    "            times = np.array(times)\n",
    "            values = np.array(values)\n",
    "            mask = (times >= start_time) & (times < end_time)\n",
    "            group2_values.extend(values[mask])\n",
    "        \n",
    "        if len(group1_values) > 0 and len(group2_values) > 0:\n",
    "            _, p_value = stats.ttest_ind(group1_values, group2_values)\n",
    "            p_values.append(p_value)\n",
    "        else:\n",
    "            p_values.append(np.nan)\n",
    "    \n",
    "    return np.array(p_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_aligned_cgm_events_by_group(\n",
    "    chinese_data: ChineseCGMData,\n",
    "    events_df: pl.DataFrame,\n",
    "    group1_filter: pl.Expr,\n",
    "    group2_filter: Optional[pl.Expr] = None,\n",
    "    group_names: tuple[str, str] = (\"Group 1\", \"Group 2\"),\n",
    "    show_individual: bool = False,\n",
    "    split_meals: bool = False,\n",
    "    split_by_calories: bool = True,\n",
    "    alpha_individual: float = 0.1,\n",
    "    figsize: tuple = (15, 10),\n",
    "    display: bool = True,\n",
    "    variable_name: str = \"Variable\"\n",
    ") -> tuple[plt.Figure, list[plt.Axes]]:\n",
    "    \"\"\"\n",
    "    Plot aligned CGM curves for events, split by clinical groups and caloric density.\n",
    "    Includes p-value plots for both between-group and within-group comparisons.\n",
    "    \"\"\"\n",
    "    # Get patient groups\n",
    "    group1_patients = chinese_data.df_metadata.filter(group1_filter)[\"Patient Number\"].to_list()\n",
    "    if group2_filter is None:\n",
    "        group2_patients = chinese_data.df_metadata.filter(~group1_filter)[\"Patient Number\"].to_list()\n",
    "    else:\n",
    "        group2_patients = chinese_data.df_metadata.filter(group2_filter)[\"Patient Number\"].to_list()\n",
    "    \n",
    "    # Adjust figure size for split view\n",
    "    if split_meals and split_by_calories:\n",
    "        figsize = (15, 25)  # Larger figure for all comparisons\n",
    "    \n",
    "    if split_meals:\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        if split_by_calories:\n",
    "            fig = plt.figure(figsize=(15, 25))  # Increased height for additional plots\n",
    "            \n",
    "            # 10 rows: main title, legend space, between-group title, 3 CGM plots, 3 between-group p-value plots, \n",
    "            # within-group title, 3 within-group p-value plots\n",
    "            gs = GridSpec(10, 3, figure=fig, \n",
    "                        height_ratios=[0.2, 0.1, 0.2, 3, 3, 1, 1, 0.2, 1, 1])\n",
    "            \n",
    "            # Create title and section headers\n",
    "            title_ax = fig.add_subplot(gs[0, :])\n",
    "            between_groups_title_ax = fig.add_subplot(gs[2, :])  # Moved up before its plots\n",
    "            within_groups_title_ax = fig.add_subplot(gs[7, :])   # Moved up before its plots\n",
    "            title_ax.axis('off')\n",
    "            between_groups_title_ax.axis('off')\n",
    "            within_groups_title_ax.axis('off')\n",
    "            \n",
    "            # Create main plots\n",
    "            cgm_axes_low = [fig.add_subplot(gs[3, i]) for i in range(3)]\n",
    "            cgm_axes_avg = [fig.add_subplot(gs[4, i]) for i in range(3)]\n",
    "            \n",
    "            # Between-group p-value plots\n",
    "            p_axes_low = [fig.add_subplot(gs[5, i]) for i in range(3)]\n",
    "            p_axes_avg = [fig.add_subplot(gs[6, i]) for i in range(3)]\n",
    "            \n",
    "            # Within-group p-value plots\n",
    "            p_axes_within_group1 = [fig.add_subplot(gs[8, i]) for i in range(3)]\n",
    "            p_axes_within_group2 = [fig.add_subplot(gs[9, i]) for i in range(3)]\n",
    "            \n",
    "            cgm_axes = {'low': cgm_axes_low, 'average': cgm_axes_avg}\n",
    "            p_axes = {'low': p_axes_low, 'average': p_axes_avg}\n",
    "            p_axes_within = {group_names[0]: p_axes_within_group1, \n",
    "                            group_names[1]: p_axes_within_group2}\n",
    "\n",
    "   \n",
    "            within_groups_title_ax.text(0.5, 0.5, \n",
    "                \"Within-Group Comparisons (Low vs Average calories within each group and meal type)\", \n",
    "                ha='center', va='center', fontsize=10)\n",
    "        else:\n",
    "            gs = GridSpec(3, 3, figure=fig, height_ratios=[0.2, 3, 1])\n",
    "            title_ax = fig.add_subplot(gs[0, :])\n",
    "            title_ax.axis('off')\n",
    "            cgm_axes = [fig.add_subplot(gs[1, i]) for i in range(3)]\n",
    "            p_axes = [fig.add_subplot(gs[2, i]) for i in range(3)]\n",
    "            \n",
    "        meal_categories = ['Breakfast', 'Lunch', 'Dinner']\n",
    "    else:\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        gs = GridSpec(3, 1, figure=fig, height_ratios=[0.2, 3, 1])\n",
    "        title_ax = fig.add_subplot(gs[0])\n",
    "        title_ax.axis('off')\n",
    "        ax_cgm = fig.add_subplot(gs[1])\n",
    "        ax_p = fig.add_subplot(gs[2])\n",
    "        cgm_axes = [ax_cgm]\n",
    "        p_axes = [ax_p]\n",
    "        meal_categories = [None]\n",
    "\n",
    "    def wrap_title(text, width=40):\n",
    "        \"\"\"Wrap title text to multiple lines if too long\"\"\"\n",
    "        import textwrap\n",
    "        return '\\n'.join(textwrap.wrap(text, width=width))\n",
    "\n",
    "    # Create title with cutoff value\n",
    "    cutoff_value = chinese_data.df_metadata[variable_name].median()\n",
    "    n_group1 = len(group1_patients)\n",
    "    n_group2 = len(group2_patients)\n",
    "    title_text = wrap_title(\n",
    "        f'CGM Patterns by {variable_name}: High {variable_name} (>{cutoff_value:.1f}, n={n_group1}) vs '\n",
    "        f'Low {variable_name} (≤{cutoff_value:.1f}, n={n_group2})'\n",
    "    )\n",
    "    \n",
    "    title_ax.text(0.5, 0.5, title_text, ha='center', va='center', fontsize=12)\n",
    "\n",
    "\n",
    "    # Prepare data structures\n",
    "    caloric_densities = ['low', 'average'] if split_by_calories else [None]\n",
    "    groups_data = {\n",
    "        cal_dens: {\n",
    "            group_name: {meal: [] for meal in meal_categories}\n",
    "            for group_name in group_names\n",
    "        }\n",
    "        for cal_dens in caloric_densities\n",
    "    }\n",
    "    \n",
    "    groups_statistics = {\n",
    "        cal_dens: {\n",
    "            group: {meal: {'means': [], 'medians': [], 'sds': []} \n",
    "                   for meal in meal_categories}\n",
    "            for group in group_names\n",
    "        }\n",
    "        for cal_dens in caloric_densities\n",
    "    }\n",
    "        # Track global ranges\n",
    "    global_time_min = float('inf')\n",
    "    global_time_max = float('-inf')\n",
    "    \n",
    "    # First pass: determine global time range\n",
    "    for row in events_df.iter_rows(named=True):\n",
    "        cgm_values = row['CGM']\n",
    "        is_before = row['is_before_food_event']\n",
    "        onset_idx = np.where(np.array(is_before) == False)[0][0]\n",
    "        timestamps = [(i - onset_idx) * 15 for i in range(len(cgm_values))]\n",
    "        \n",
    "        global_time_min = min(global_time_min, min(timestamps))\n",
    "        global_time_max = max(global_time_max, max(timestamps))\n",
    "    \n",
    "    # Create standard time points\n",
    "    standard_times = np.arange(global_time_min, global_time_max + 15, 15)\n",
    "    \n",
    "    # Second pass: sort events into groups\n",
    "    for row in events_df.iter_rows(named=True):\n",
    "        patient = row['Patient Number']\n",
    "        current_category = row['event_type'] if split_meals else None\n",
    "        caloric_density = row.get('meal_caloric_density', None)\n",
    "        \n",
    "        if split_by_calories and caloric_density not in ['low', 'average']:\n",
    "            continue\n",
    "            \n",
    "        if patient in group1_patients:\n",
    "            group = group_names[0]\n",
    "        elif patient in group2_patients:\n",
    "            group = group_names[1]\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        cgm_values = row['CGM']\n",
    "        is_before = row['is_before_food_event']\n",
    "        onset_idx = np.where(np.array(is_before) == False)[0][0]\n",
    "        timestamps = [(i - onset_idx) * 15 for i in range(len(cgm_values))]\n",
    "        \n",
    "        cal_dens = caloric_density if split_by_calories else None\n",
    "        groups_data[cal_dens][group][current_category].append((timestamps, cgm_values))\n",
    "    \n",
    "    # Calculate statistics and determine global CGM range\n",
    "    global_cgm_min = float('inf')\n",
    "    global_cgm_max = float('-inf')\n",
    "    \n",
    "    for cal_dens in caloric_densities:\n",
    "        for group in group_names:\n",
    "            for meal_category in meal_categories:\n",
    "                all_series = groups_data[cal_dens][group][meal_category]\n",
    "                if not all_series:\n",
    "                    continue\n",
    "                \n",
    "                values_matrix = np.full((len(all_series), len(standard_times)), np.nan)\n",
    "                \n",
    "                for idx, (times, values) in enumerate(all_series):\n",
    "                    interp_values = np.interp(standard_times, times, values)\n",
    "                    values_matrix[idx] = interp_values\n",
    "                \n",
    "                mean_curve = np.nanmean(values_matrix, axis=0)\n",
    "                median_curve = np.nanmedian(values_matrix, axis=0)\n",
    "                std_curve = np.nanstd(values_matrix, axis=0)\n",
    "                \n",
    "                groups_statistics[cal_dens][group][meal_category]['means'] = mean_curve\n",
    "                groups_statistics[cal_dens][group][meal_category]['medians'] = median_curve\n",
    "                groups_statistics[cal_dens][group][meal_category]['sds'] = std_curve\n",
    "                \n",
    "                if show_individual:\n",
    "                    global_cgm_min = min(global_cgm_min, np.nanmin(values_matrix))\n",
    "                    global_cgm_max = max(global_cgm_max, np.nanmax(values_matrix))\n",
    "                else:\n",
    "                    global_cgm_min = min(global_cgm_min, np.nanmin(mean_curve - std_curve))\n",
    "                    global_cgm_max = max(global_cgm_max, np.nanmax(mean_curve + std_curve))\n",
    "    \n",
    "    # Add padding to CGM range\n",
    "    cgm_range = global_cgm_max - global_cgm_min\n",
    "    global_cgm_min -= cgm_range * 0.05\n",
    "    global_cgm_max += cgm_range * 0.05\n",
    "    \n",
    "    # Colors for groups\n",
    "    colors = {group_names[0]: 'blue', group_names[1]: 'red'}\n",
    "    \n",
    "    # Plot each caloric density category\n",
    "    for cal_dens in caloric_densities:\n",
    "        current_cgm_axes = cgm_axes[cal_dens] if split_by_calories and split_meals else cgm_axes\n",
    "        current_p_axes = p_axes[cal_dens] if split_by_calories and split_meals else p_axes\n",
    "        \n",
    "        for ax_idx, (ax_cgm, ax_p, meal_category) in enumerate(zip(current_cgm_axes, current_p_axes, meal_categories)):\n",
    "            group1_data = groups_data[cal_dens][group_names[0]][meal_category]\n",
    "            group2_data = groups_data[cal_dens][group_names[1]][meal_category]\n",
    "            \n",
    "            # Get event counts for title\n",
    "            group1_count = len(group1_data)\n",
    "            group2_count = len(group2_data)\n",
    "            total_events = group1_count + group2_count\n",
    "            \n",
    "            for group in group_names:\n",
    "                all_series = groups_data[cal_dens][group][meal_category]\n",
    "                if not all_series:\n",
    "                    continue\n",
    "                \n",
    "                color = colors[group]\n",
    "                \n",
    "                if show_individual:\n",
    "                    for times, values in all_series:\n",
    "                        interp_values = np.interp(standard_times, times, values)\n",
    "                        ax_cgm.plot(standard_times, interp_values, '-', \n",
    "                                  color=color, alpha=alpha_individual)\n",
    "                \n",
    "                stats = groups_statistics[cal_dens][group][meal_category]\n",
    "                mean_curve = stats['means']\n",
    "                std_curve = stats['sds']\n",
    "                \n",
    "                ax_cgm.plot(standard_times, mean_curve, '-', \n",
    "                           color=color, linewidth=2, \n",
    "                           label=f'{group} (n={len(all_series)})')\n",
    "                ax_cgm.fill_between(standard_times, \n",
    "                                  mean_curve - std_curve, \n",
    "                                  mean_curve + std_curve,\n",
    "                                  color=color, alpha=0.2)\n",
    "            \n",
    "            # Plot between-group p-values\n",
    "            if group1_data and group2_data:\n",
    "                p_values = calculate_hourly_p_values(group1_data, group2_data, standard_times)\n",
    "                hours = np.arange(int(standard_times[0] / 60), int(standard_times[-1] / 60) + 1)\n",
    "                \n",
    "                significance = 1 - p_values\n",
    "                \n",
    "                ax_p.plot(hours * 60, significance, 'k-', linewidth=2, label='Significance')\n",
    "                ax_p.axhline(y=0.95, color='r', linestyle='--', alpha=0.5, label='p=0.05')\n",
    "                ax_p.fill_between(hours * 60, 0, significance, alpha=0.2)\n",
    "                \n",
    "                ax_p.set_ylim(0.8, 1)\n",
    "                ax_p.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Common plot elements\n",
    "            ax_cgm.axvline(x=0, color='k', linestyle='--', label='Event Onset')\n",
    "            ax_cgm.set_ylim(global_cgm_min, global_cgm_max)\n",
    "            ax_cgm.set_xlim(standard_times[0], standard_times[-1])\n",
    "            \n",
    "            for ax in [ax_cgm, ax_p]:\n",
    "                ax.set_xlabel('Minutes from Event')\n",
    "                xticks = np.arange(standard_times[0], standard_times[-1] + 1, 120)\n",
    "                ax.set_xticks(xticks)\n",
    "                ax.set_xticklabels([str(int(x)) for x in xticks])\n",
    "            \n",
    "            if ax_idx == 0:\n",
    "                ax_cgm.set_ylabel('CGM Value')\n",
    "                ax_p.set_ylabel('Statistical Significance (1-p)')\n",
    "            \n",
    "            # Create title with event counts\n",
    "            if meal_category:\n",
    "                cal_dens_text = f\" ({cal_dens.capitalize()} Calories)\" if cal_dens else \"\"\n",
    "                title = wrap_title(\n",
    "                    f'{meal_category} Events{cal_dens_text} High: {group1_count}, Low: {group2_count}'\n",
    "                )\n",
    "            else:\n",
    "                title = wrap_title(\n",
    "                    f'All Events High: {group1_count}, Low: {group2_count}'\n",
    "                )\n",
    "            \n",
    "            ax_cgm.set_title(title)\n",
    "            ax_cgm.grid(True, alpha=0.3)\n",
    "            ax_p.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add within-group p-value plots if using split view\n",
    "    if split_by_calories and split_meals:\n",
    "        for group in group_names:\n",
    "            for ax_idx, (ax_p, meal_category) in enumerate(zip(p_axes_within[group], meal_categories)):\n",
    "                low_cal_data = groups_data['low'][group][meal_category]\n",
    "                avg_cal_data = groups_data['average'][group][meal_category]\n",
    "                \n",
    "                if low_cal_data and avg_cal_data:\n",
    "                    p_values = calculate_hourly_p_values(low_cal_data, avg_cal_data, standard_times)\n",
    "                    hours = np.arange(int(standard_times[0] / 60), int(standard_times[-1] / 60) + 1)\n",
    "                    \n",
    "                    significance = 1 - p_values\n",
    "                    \n",
    "                    ax_p.plot(hours * 60, significance, 'k-', linewidth=2)\n",
    "                    ax_p.axhline(y=0.95, color='r', linestyle='--', alpha=0.5)\n",
    "                    ax_p.fill_between(hours * 60, 0, significance, alpha=0.2)\n",
    "                    \n",
    "                    ax_p.set_ylim(0.8, 1)\n",
    "                    ax_p.grid(True, alpha=0.3)\n",
    "                    \n",
    "                    if ax_idx == 0:\n",
    "                        ax_p.set_ylabel(f'{group}\\nSignificance (1-p)')\n",
    "                    ax_p.set_xlabel('Minutes from Event')\n",
    "                    \n",
    "                    # Add title with sample sizes\n",
    "                    n_low = len(low_cal_data)\n",
    "                    n_avg = len(avg_cal_data)\n",
    "                    ax_p.set_title(f'{meal_category}\\n(Low: {n_low}, Avg: {n_avg})')\n",
    "                else:\n",
    "                    print(f\"Skipping within-group p-value calculation for {group}, {meal_category}\")\n",
    "\n",
    "    # Create a single legend\n",
    "    handles, labels = (cgm_axes['low'] if split_by_calories and split_meals else cgm_axes)[0].get_legend_handles_labels()\n",
    "    p_handles, p_labels = (p_axes['low'] if split_by_calories and split_meals else p_axes)[0].get_legend_handles_labels()\n",
    "    \n",
    "    unique_labels = []\n",
    "    unique_handles = []\n",
    "    for handle, label in zip(handles + p_handles, labels + p_labels):\n",
    "        if label not in unique_labels:\n",
    "            unique_labels.append(label)\n",
    "            unique_handles.append(handle)\n",
    "    \n",
    "    # Move legend between title and plots\n",
    "    if split_by_calories and split_meals:\n",
    "        legend_y = 0.88\n",
    "    else:\n",
    "        legend_y = 0.85\n",
    "    \n",
    "    fig.legend(unique_handles, unique_labels, \n",
    "              loc='upper center', bbox_to_anchor=(0.5, legend_y), ncol=5)\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    if split_by_calories and split_meals:\n",
    "        plt.subplots_adjust(top=0.92, bottom=0.05, hspace=0.6)\n",
    "    else:\n",
    "        plt.subplots_adjust(top=0.92)\n",
    "\n",
    "    if display:\n",
    "        plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get events data\n",
    "\n",
    "# Plot with groups based on some clinical variable\n",
    "fig = plot_aligned_cgm_events_by_group(\n",
    "    chinese_data,\n",
    "    events_with_calories,\n",
    "    group1_filter=pl.col(\"HbA1c (mmol/mol)\") > chinese_data.df_metadata[\"HbA1c (mmol/mol)\"].median(),\n",
    "    group_names=(\"High HbA1c\", \"Low HbA1c\"),\n",
    "    split_meals=True,\n",
    "    variable_name=\"HbA1c (mmol/mol)\",\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cgm_statistics(\n",
    "    chinese_data: ChineseCGMData,\n",
    "    group1_filter: pl.Expr,\n",
    "    group2_filter: Optional[pl.Expr] = None,\n",
    ") -> tuple[dict[str, list[dict]], int]:\n",
    "    \"\"\"\n",
    "    Calculate hourly statistics between two groups.\n",
    "    Returns hourly statistics and number of significant hours.\n",
    "    \"\"\"\n",
    "    # Get daily data\n",
    "    cgm_resolution_df = chinese_data.get_cgm_data_at_resolution(\"1d\")\n",
    "    \n",
    "    # Filter patients based on metadata\n",
    "    group1_patients = chinese_data.df_metadata.filter(group1_filter)[\"Patient Number\"].to_list()\n",
    "    if group2_filter is None:\n",
    "        group2_patients = chinese_data.df_metadata.filter(~group1_filter)[\"Patient Number\"].to_list()\n",
    "    else:\n",
    "        group2_patients = chinese_data.df_metadata.filter(group2_filter)[\"Patient Number\"].to_list()\n",
    "    \n",
    "    def process_patient_data(patient_df):\n",
    "        daily_data = {hour: [] for hour in range(24)}\n",
    "        for row in patient_df.iter_rows(named=True):\n",
    "            for time, value in zip(row[\"cgm_time_stamp\"], row[\"CGM\"]):\n",
    "                hour = time.hour\n",
    "                daily_data[hour].append(value)\n",
    "                \n",
    "        hourly_means = []\n",
    "        for hour in range(24):\n",
    "            if daily_data[hour]:\n",
    "                hourly_means.append(np.mean(daily_data[hour]))\n",
    "            else:\n",
    "                hourly_means.append(np.nan)\n",
    "        return hourly_means\n",
    "    \n",
    "    # Process each group\n",
    "    group1_curves = {}\n",
    "    group2_curves = {}\n",
    "    \n",
    "    for patient in group1_patients:\n",
    "        patient_df = cgm_resolution_df.filter(pl.col(\"Patient Number\") == patient)\n",
    "        group1_curves[patient] = process_patient_data(patient_df)\n",
    "    \n",
    "    for patient in group2_patients:\n",
    "        patient_df = cgm_resolution_df.filter(pl.col(\"Patient Number\") == patient)\n",
    "        group2_curves[patient] = process_patient_data(patient_df)\n",
    "    \n",
    "    # Convert to arrays for statistics\n",
    "    group1_array = np.array(list(group1_curves.values()))\n",
    "    group2_array = np.array(list(group2_curves.values()))\n",
    "    \n",
    "    # Calculate hourly statistics and t-tests\n",
    "    hourly_stats = []\n",
    "    for hour in range(24):\n",
    "        g1_hour = group1_array[:, hour]\n",
    "        g2_hour = group2_array[:, hour]\n",
    "        \n",
    "        # Remove NaN values for t-test\n",
    "        g1_clean = g1_hour[~np.isnan(g1_hour)]\n",
    "        g2_clean = g2_hour[~np.isnan(g2_hour)]\n",
    "        \n",
    "        if len(g1_clean) > 0 and len(g2_clean) > 0:\n",
    "            t_stat, p_val = stats.ttest_ind(g1_clean, g2_clean)\n",
    "        else:\n",
    "            t_stat, p_val = np.nan, np.nan\n",
    "            \n",
    "        hourly_stats.append({\n",
    "            'hour': hour,\n",
    "            'g1_mean': np.nanmean(g1_hour),\n",
    "            'g1_std': np.nanstd(g1_hour),\n",
    "            'g2_mean': np.nanmean(g2_hour),\n",
    "            'g2_std': np.nanstd(g2_hour),\n",
    "            'p_value': p_val\n",
    "        })\n",
    "    \n",
    "    significant_hours = sum(1 for stat in hourly_stats if stat['p_value'] < 0.05)\n",
    "    \n",
    "    return {\n",
    "        'hourly_stats': hourly_stats,\n",
    "        'group1_curves': group1_curves,\n",
    "        'group2_curves': group2_curves,\n",
    "        'group_sizes': (len(group1_curves), len(group2_curves))\n",
    "    }, significant_hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_significant_vars(\n",
    "    chinese_data: ChineseCGMData,\n",
    "    excluded_vars: list[str] = None\n",
    ") -> tuple[list[str], dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Take the variable names (strings) in order of statistical significance\n",
    "    \n",
    "    Args:\n",
    "        chinese_data: ChineseCGMData object containing the data\n",
    "        excluded_vars: List of numeric variables to exclude from analysis\n",
    "    \n",
    "    Returns:\n",
    "        list of variables ordered by significance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define numeric columns\n",
    "    numeric_cols = [\n",
    "        \"Fasting Plasma Glucose (mg/dl)\",\n",
    "        \"2-hour Postprandial Plasma Glucose (mg/dl)\",\n",
    "        \"Fasting C-peptide (nmol/L)\",\n",
    "        \"2-hour Postprandial C-peptide (nmol/L)\",\n",
    "        \"Fasting Insulin (pmol/L)\",\n",
    "        \"2-hour Postprandial insulin (pmol/L)\",\n",
    "        \"HbA1c (mmol/mol)\",\n",
    "        \"Glycated Albumin (%)\",\n",
    "        \"Total Cholesterol (mmol/L)\",\n",
    "        \"Triglyceride (mmol/L)\",\n",
    "        \"High-Density Lipoprotein Cholesterol (mmol/L)\",\n",
    "        \"Low-Density Lipoprotein Cholesterol (mmol/L)\",\n",
    "        \"Creatinine (umol/L)\",\n",
    "        \"Estimated Glomerular Filtration Rate (ml/min/1.73m2)\",\n",
    "        \"Uric Acid (mmol/L)\",\n",
    "        \"Blood Urea Nitrogen (mmol/L)\",\n",
    "        \"Age (years)\",\n",
    "        \"Height (m)\",\n",
    "        \"Weight (kg)\",\n",
    "        \"BMI (kg/m2)\",\n",
    "    ]\n",
    "    \n",
    "    # Remove excluded variables\n",
    "    if excluded_vars:\n",
    "        numeric_cols = [col for col in numeric_cols if col not in excluded_vars]\n",
    "    \n",
    "    # Filter to only existing columns with non-null values\n",
    "    valid_cols = []\n",
    "    for col in numeric_cols:\n",
    "        if col in chinese_data.df_metadata.columns:\n",
    "            if chinese_data.df_metadata[col].null_count() < len(chinese_data.df_metadata):\n",
    "                valid_cols.append(col)\n",
    "    \n",
    "    # Calculate statistics for all variables\n",
    "    var_significance = {}\n",
    "    var_stats = {}\n",
    "    for var in valid_cols:\n",
    "        median_val = chinese_data.df_metadata[var].median()\n",
    "        stats, significant_hours = calculate_cgm_statistics(\n",
    "            chinese_data,\n",
    "            group1_filter=pl.col(var) > median_val\n",
    "        )\n",
    "        var_significance[var] = significant_hours\n",
    "        var_stats[var] = stats\n",
    "    \n",
    "    # Sort variables by significance\n",
    "    sorted_vars = sorted(var_significance.items(), key=lambda x: x[1], reverse=True)\n",
    "    ordered_vars = [v[0] for v in sorted_vars]\n",
    "\n",
    "    return ordered_vars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ordered_vars = get_significant_vars(chinese_data)\n",
    "for column in ordered_vars:\n",
    "    plot_aligned_cgm_events_by_group(\n",
    "    chinese_data,\n",
    "    events_with_calories,\n",
    "    group1_filter=pl.col(column) > chinese_data.df_metadata[column].median(),\n",
    "    group_names=(f\"High {column}\", f\"Low {column}\"),\n",
    "    split_meals=True,\n",
    "    variable_name=column,\n",
    "    split_by_calories=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
